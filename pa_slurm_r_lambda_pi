#!/bin/bash
#SBATCH -J tjarray
#SBATCH --gres=gpu:1
#SBATCH --output=/tmp-network/user/tparshak/plambda_%A_%a.log
#SBATCH -p papago
#SBATCH -t 200:00:00
#SBATCH --mem=40000
#SBATCH --cpus-per-task=2
#SBATCH --constraint="gpu_v100"

hostname
srun hostname
srun nvidia-smi -L
cd /tmp-network/user/tparshak/exp_gams

seqlen=( 30 )
motif=( 4 )
#dssize=( 5000 10000 20000 )
dssize=( 1000 )
#(motif, supermotif, submotif__2, 1st bit==0, 10101_len_m, 1001001_le_m_2, 00110011_len_m__2)
#feats=( '01011111' '11111111' )
feats=( 's001111' '1001111' )
#feats=( '1111111'  )
#train_reg=( 'rs' )
train_reg=( 'rs' 'snis_r' )
mtypes=( 'm' )

for n in ${seqlen[@]}; do
	for m in ${motif[@]}; do
		for ds in ${dssize[@]}; do
			for f in ${feats[@]}; do
				for tr in ${train_reg[@]}; do
					for mt in ${mtypes[@]}; do
						echo "$ds||$m||$n||$f||$tr||$mt"
						python -u r_plambda_distill_pitheta.py --n ${n} --ds_size ${ds} --motif ${m} --job ${SLURM_JOB_ID} --feat ${f} --train ${tr} --mtype ${mt} --restore yes --distill_size 20000
						echo "$ds||$m||$n||$f||$tr||$mt"
					done
				done
			done
		done
	done
done
scontrol show job ${SLURM_ARRAY_JOB_ID}
echo "SLURM_JOBID=$SLURM_JOBID"
echo "SLURM_ARRAY_JOB_ID=$SLURM_ARRAY_JOB_ID"
echo "SLURM_ARRAY_TASK_ID=$SLURM_ARRAY_TASK_ID"
cp /tmp-network/user/tparshak/plambda_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.log /home/tparshak/logs/plambda_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.log
rm -f /tmp-network/user/tparshak/plambda_${SLURM_ARRAY_JOB_ID}_${SLURM_ARRAY_TASK_ID}.log

